{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e258335-6796-4ebf-be19-f94a5da1ef31",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_account_name = \"mlopsdatastore\"\n",
    "container_name = \"titanic\"\n",
    "storage_account_key = dbutils.secrets.get(scope=\"creds\", key=\"azuretoken\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3897edae-a438-42db-b981-bc035ec51247",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set the Spark configuration\n",
    "spark.conf.set(\n",
    "  \"fs.azure.account.key.{0}.blob.core.windows.net\".format(storage_account_name),\n",
    "  storage_account_key\n",
    ")\n",
    "\n",
    "# Mount the Blob Storage container\n",
    "try:\n",
    "  dbutils.fs.mount(\n",
    "    source = \"wasbs://{0}@{1}.blob.core.windows.net\".format(container_name, storage_account_name),\n",
    "    mount_point = \"/mnt/datamount/data\",\n",
    "    extra_configs = {\"fs.azure.account.key.{0}.blob.core.windows.net\".format(storage_account_name): storage_account_key}\n",
    "  )\n",
    "except Exception as e:\n",
    "  print(\"Already Mounted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f623b9f-e04c-482f-ac67-2a84e8058766",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the file path\n",
    "file_path = \"/mnt/datamount/data/train.csv\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "train_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7b34c29-3c00-4f57-984c-a34c8dedb70c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PandasToPySpark\").getOrCreate()\n",
    "\n",
    "#'train_df' is a PySpark DataFrame\n",
    "train_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"/mnt/datamount/data/train.csv\")\n",
    "\n",
    "# Drop the 'Cabin' column\n",
    "train_df = train_df.drop('Cabin')\n",
    "\n",
    "# Drop rows with missing values\n",
    "train_df = train_df.na.drop()\n",
    "\n",
    "# Drop duplicates\n",
    "train_df = train_df.dropDuplicates()\n",
    "\n",
    "# Calculate the Z-scores for 'Fare' column\n",
    "w = Window.orderBy(F.lit(0))\n",
    "train_df = train_df.withColumn(\"FareZScore\", F.abs(F.col(\"Fare\") - F.avg(\"Fare\").over(w)) / F.stddev(\"Fare\").over(w))\n",
    "\n",
    "# Set a Z-score threshold for outlier removal\n",
    "threshold = 0.1\n",
    "\n",
    "# Create a new DataFrame with outliers removed\n",
    "df_no_outliers = train_df.filter(train_df['FareZScore'] <= threshold).drop(\"FareZScore\")\n",
    "\n",
    "df_no_outliers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98e93a88-75c9-402d-86e9-91a698420c9b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_no_outliers.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/datamount/delta_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52916cf6-9e00-4c60-b83a-10f13735773e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f6ad638-eb8f-4b66-812d-e58dc507e855",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ETL",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
