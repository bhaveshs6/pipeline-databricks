{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55d221d7-ebc1-4b63-9a9f-39704fb363f9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Step 1: Data Preparation and ETL Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efeacdc6-f9e5-4203-bc8a-f5d9a025cbcb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e258335-6796-4ebf-be19-f94a5da1ef31",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_account_name = \"mlopsdatastore\"\n",
    "container_name = \"titanic\"\n",
    "storage_account_key = dbutils.secrets.get(scope=\"tokens\", key=\"aztoken\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3897edae-a438-42db-b981-bc035ec51247",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set the Spark configuration\n",
    "spark.conf.set(\n",
    "  \"fs.azure.account.key.{0}.blob.core.windows.net\".format(storage_account_name),\n",
    "  storage_account_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "544fdc14-0791-4db6-aabb-865d7d2980c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mount the Blob Storage container\n",
    "try:\n",
    "  dbutils.fs.mount(\n",
    "    source = \"wasbs://{0}@{1}.blob.core.windows.net\".format(container_name, storage_account_name),\n",
    "    mount_point = \"/mnt/datamount/data\",\n",
    "    extra_configs = {\"fs.azure.account.key.{0}.blob.core.windows.net\".format(storage_account_name): storage_account_key}\n",
    "  )\n",
    "except Exception as e:\n",
    "  print(\"Already Mounted or Wrong Credentials\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f623b9f-e04c-482f-ac67-2a84e8058766",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the file path\n",
    "file_path = \"/mnt/datamount/data/train.csv\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "train_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7de7ab41-01b0-4050-9245-0bfcb47bfa7d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Importing Modules for ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5082a8b2-4a4d-4fe5-98da-3c4f79b0d396",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "import dlt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7b34c29-3c00-4f57-984c-a34c8dedb70c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "spark = SparkSession.builder.appName(\"PandasToPySpark\").getOrCreate()\n",
    "\n",
    "#'train_df' is a PySpark DataFrame\n",
    "train_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"/mnt/datamount/data/train.csv\")\n",
    "\n",
    "# Drop the 'Cabin' column\n",
    "train_df = train_df.drop('Cabin')\n",
    "\n",
    "# Drop rows with missing values\n",
    "train_df = train_df.na.drop()\n",
    "\n",
    "# Drop duplicates\n",
    "train_df = train_df.dropDuplicates()\n",
    "\n",
    "# Calculate the Z-scores for 'Fare' column\n",
    "w = Window.orderBy(F.lit(0))\n",
    "train_df = train_df.withColumn(\"FareZScore\", F.abs(F.col(\"Fare\") - F.avg(\"Fare\").over(w)) / F.stddev(\"Fare\").over(w))\n",
    "\n",
    "# Set a Z-score threshold for outlier removal\n",
    "threshold = 0.1\n",
    "\n",
    "# Create a new DataFrame with outliers removed\n",
    "df_no_outliers = train_df.filter(train_df['FareZScore'] <= threshold).drop(\"FareZScore\")\n",
    "\n",
    "df_no_outliers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6d4e7bb-71cc-4797-9f9b-c4a2e6481dd9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=\"delta_live_table\",\n",
    "    # schema=\"passenger_id INT, survived INT, pclass INT, name STRING, sex STRING, age FLOAT, sibsp INT, parch INT, ticket STRING, fare FLOAT, cabin STRING, embarked STRING\",\n",
    ")\n",
    "def train_df():\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"/mnt/datamount/data/train.csv\")\n",
    "\n",
    "    # Drop the 'Cabin' column\n",
    "    df = df.drop('Cabin')\n",
    "\n",
    "    # Drop rows with missing values\n",
    "    df = df.na.drop()\n",
    "\n",
    "    # Drop duplicates\n",
    "    df = df.dropDuplicates()\n",
    "\n",
    "    # Calculate the Z-scores for 'Fare' column\n",
    "    w = Window.orderBy(F.lit(0))\n",
    "    df = df.withColumn(\"FareZScore\", F.abs(F.col(\"Fare\") - F.avg(\"Fare\").over(w)) / F.stddev(\"Fare\").over(w))\n",
    "\n",
    "    # Set a Z-score threshold for outlier removal\n",
    "    threshold = 0.1\n",
    "\n",
    "    # Filter out outliers\n",
    "    df = df.filter(df['FareZScore'] <= threshold)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98e93a88-75c9-402d-86e9-91a698420c9b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    df_no_outliers.write.format(\"delta\").save(\"/mnt/datamount/delta_table\")\n",
    "except:\n",
    "    df_no_outliers.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/datamount/delta_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52916cf6-9e00-4c60-b83a-10f13735773e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc2a4893-ff03-4fe7-bc8c-99ad1fcf7c8d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install dlt --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12b4cb74-17c0-406e-beb4-97deabf48d34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3556767246371059,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ETL",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
